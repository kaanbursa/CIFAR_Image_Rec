{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
    "\n",
    "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class\n",
    "\n",
    "Data Source: https://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Path for file\n",
    "CIFAR_DIR = './cifar-10-batches-py/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file,'rb') as f:\n",
    "        cifar_dict = pickle.load(f)\n",
    "    return cifar_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dirs = ['batches.meta','data_batch_1','data_batch_2','data_batch_3','data_batch_4','data_batch_5','test_batch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = [0,1,2,3,4,5,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,direc in zip(all_data,dirs):\n",
    "    all_data[i] = unpickle(CIFAR_DIR+direc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_meta = all_data[0]\n",
    "data_batch1 = all_data[1]\n",
    "data_batch2 = all_data[2]\n",
    "data_batch3 = all_data[3]\n",
    "data_batch4 = all_data[4]\n",
    "data_batch5 = all_data[5]\n",
    "test_batch = all_data[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label_names': ['airplane',\n",
       "  'automobile',\n",
       "  'bird',\n",
       "  'cat',\n",
       "  'deer',\n",
       "  'dog',\n",
       "  'frog',\n",
       "  'horse',\n",
       "  'ship',\n",
       "  'truck'],\n",
       " 'num_cases_per_batch': 10000,\n",
       " 'num_vis': 3072}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'labels', 'batch_label', 'filenames']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_batch1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_batch1['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11849fc90>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n78NzaabVgQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(vec, vals=10):\n",
    "    '''\n",
    "    For use to one-hot encode the 10- possible labels\n",
    "    '''\n",
    "    n = len(vec)\n",
    "    out = np.zeros((n, vals))\n",
    "    out[range(n), vec] = 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CifarHelper():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.i = 0\n",
    "        \n",
    "        # Grabs a list of all the data batches for training\n",
    "        self.all_train_batches = [data_batch1,data_batch2,data_batch3,data_batch4,data_batch5]\n",
    "        # Grabs a list of all the test batches (really just one batch)\n",
    "        self.test_batch = [test_batch]\n",
    "        \n",
    "        # Intialize some empty variables for later on\n",
    "        self.training_images = None\n",
    "        self.training_labels = None\n",
    "        \n",
    "        self.test_images = None\n",
    "        self.test_labels = None\n",
    "    \n",
    "    def set_up_images(self):\n",
    "        \n",
    "        print(\"Setting Up Training Images and Labels\")\n",
    "        \n",
    "        # Vertically stacks the training images\n",
    "        self.training_images = np.vstack([d[b\"data\"] for d in self.all_train_batches])\n",
    "        train_len = len(self.training_images)\n",
    "        \n",
    "        # Reshapes and normalizes training images\n",
    "        self.training_images = self.training_images.reshape(train_len,3,32,32).transpose(0,2,3,1)/255\n",
    "        # One hot Encodes the training labels (e.g. [0,0,0,1,0,0,0,0,0,0])\n",
    "        self.training_labels = one_hot_encode(np.hstack([d[b\"labels\"] for d in self.all_train_batches]), 10)\n",
    "        \n",
    "        print(\"Setting Up Test Images and Labels\")\n",
    "        \n",
    "        # Vertically stacks the test images\n",
    "        self.test_images = np.vstack([d[b\"data\"] for d in self.test_batch])\n",
    "        test_len = len(self.test_images)\n",
    "        \n",
    "        # Reshapes and normalizes test images\n",
    "        self.test_images = self.test_images.reshape(test_len,3,32,32).transpose(0,2,3,1)/255\n",
    "        # One hot Encodes the test labels (e.g. [0,0,0,1,0,0,0,0,0,0])\n",
    "        self.test_labels = one_hot_encode(np.hstack([d[b\"labels\"] for d in self.test_batch]), 10)\n",
    "\n",
    "        \n",
    "    def next_batch(self, batch_size):\n",
    "        # Note that the 100 dimension in the reshape call is set by an assumed batch size of 100\n",
    "        x = self.training_images[self.i:self.i+batch_size].reshape(100,32,32,3)\n",
    "        y = self.training_labels[self.i:self.i+batch_size]\n",
    "        self.i = (self.i + batch_size) % len(self.training_images)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Up Training Images and Labels\n",
      "Setting Up Test Images and Labels\n"
     ]
    }
   ],
   "source": [
    "ch = CifarHelper()\n",
    "ch.set_up_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32,shape=[None,32,32,3])\n",
    "y_true = tf.placeholder(tf.float32,shape=[None,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hold_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialized weights\n",
    "def init_weights(shape):\n",
    "    init_rand_dist = tf.truncated_normal(shape=shape,stddev=0.1)\n",
    "    return tf.Variable(init_rand_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initilaze biases\n",
    "def init_bias(shape):\n",
    "    init_biases = tf.constant(0.1,shape=shape)\n",
    "    return tf.Variable(init_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create 3d conc\n",
    "def conv2d(x,W):\n",
    "    \n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create max pooling 2x2\n",
    "def max_pool_2b2(x):\n",
    "    \n",
    "    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Activation\n",
    "def convolutional_layer(x,shape):\n",
    "    \n",
    "    W = init_weights(shape)\n",
    "    b = init_bias([shape[3]])\n",
    "    \n",
    "    return tf.nn.relu(conv2d(x,W)+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normal_full_layer(input_layer,size):\n",
    "    input_size = int(input_layer.get_shape()[1])\n",
    "    W = init_weights([input_size,size])\n",
    "    b = init_bias([size])\n",
    "    return tf.matmul(input_layer,W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo_1 = convolutional_layer(x,shape=[4,4,3,32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo_1_pool = max_pool_2b2(convo_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo_2 = convolutional_layer(convo_1_pool,shape=[4,4,32,64])\n",
    "convo_2_pool = max_pool_2b2(convo_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo_2_flat = tf.reshape(convo_2_pool,[-1,8*8*64])\n",
    "full_layer_one = tf.nn.relu(normal_full_layer(convo_2_flat,1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_one_dropout = tf.nn.dropout(full_layer_one,keep_prob=hold_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss function\n",
    "y_pred = normal_full_layer(full_one_dropout,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true,logits=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "train = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ON STEP: 0\n",
      "ACCURACY: \n",
      "0.1156\n",
      "\n",
      "\n",
      "ON STEP: 100\n",
      "ACCURACY: \n",
      "0.1398\n",
      "\n",
      "\n",
      "ON STEP: 200\n",
      "ACCURACY: \n",
      "0.1529\n",
      "\n",
      "\n",
      "ON STEP: 300\n",
      "ACCURACY: \n",
      "0.152\n",
      "\n",
      "\n",
      "ON STEP: 400\n",
      "ACCURACY: \n",
      "0.1446\n",
      "\n",
      "\n",
      "ON STEP: 500\n",
      "ACCURACY: \n",
      "0.1596\n",
      "\n",
      "\n",
      "ON STEP: 600\n",
      "ACCURACY: \n",
      "0.1642\n",
      "\n",
      "\n",
      "ON STEP: 700\n",
      "ACCURACY: \n",
      "0.1659\n",
      "\n",
      "\n",
      "ON STEP: 800\n",
      "ACCURACY: \n",
      "0.1486\n",
      "\n",
      "\n",
      "ON STEP: 900\n",
      "ACCURACY: \n",
      "0.1728\n",
      "\n",
      "\n",
      "ON STEP: 1000\n",
      "ACCURACY: \n",
      "0.1669\n",
      "\n",
      "\n",
      "ON STEP: 1100\n",
      "ACCURACY: \n",
      "0.171\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(1200):\n",
    "        \n",
    "        batch_x, batch_y = ch.next_batch(100)\n",
    "        \n",
    "        sess.run(train,feed_dict={x:batch_x,y_true:batch_y,hold_prob:0.5})\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            print(\"ON STEP: {}\".format(i))\n",
    "            print(\"ACCURACY: \")\n",
    "            matches = tf.equal(tf.argmax(y_pred,1),tf.argmax(y_true,1))\n",
    "            \n",
    "            acc = tf.reduce_mean(tf.cast(matches,tf.float32))\n",
    "            \n",
    "            print(sess.run(acc,feed_dict={x:ch.test_images,y_true:ch.test_labels,hold_prob:1.0}))\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
